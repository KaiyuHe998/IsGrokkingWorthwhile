{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "def build_dicts(entities):\n",
    "    entity2ind = dict()\n",
    "    ind2entity = []\n",
    "    for i in range(len(entities)):\n",
    "        entity = entities[i]\n",
    "        if not (entity in ind2entity):\n",
    "            ind2entity.append(entity)\n",
    "            entity2ind[entity] = len(ind2entity) - 1\n",
    "    return ind2entity, entity2ind\n",
    "\n",
    "def choose(arr, ratio_or_count):\n",
    "    if type(ratio_or_count) == float:\n",
    "        num = round(ratio_or_count*len(arr))\n",
    "    elif type(ratio_or_count) == int:\n",
    "        num = ratio_or_count\n",
    "    else:\n",
    "         assert False\n",
    "    if num >= len(arr):\n",
    "        return arr\n",
    "    rand_inds = np.random.choice(len(arr), num, replace=False).tolist()\n",
    "    return [arr[i] for i in rand_inds]\n",
    "    \n",
    "def split(arr, ratio_or_count):\n",
    "    if type(ratio_or_count) == float:\n",
    "        num = round(ratio_or_count*len(arr))\n",
    "    elif type(ratio_or_count) == int:\n",
    "        num = ratio_or_count\n",
    "    else:\n",
    "         assert False\n",
    "    train, test = [], []\n",
    "    rand_inds = np.random.choice(len(arr), num, replace=False).tolist()\n",
    "    for i in tqdm(range(len(arr))):\n",
    "        if i in rand_inds:\n",
    "            train.append(arr[i])\n",
    "        else:\n",
    "            test.append(arr[i])\n",
    "    return [train, test]\n",
    "\n",
    "def form_items(c, t):\n",
    "    input_text = \"\".join(c)\n",
    "    target_text = input_text + \"\".join([t, \"</a>\"])\n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b533c58a2d5c4e3bb518545cb5557f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b750303846754f75a65a87c27398b23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dbe01bc2b84651a8f56631804b605e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_dataset(num_entities, num_relations, out_degree=20, split_train_inferred=False):\n",
    " \n",
    "    entities = [\"<e_{}>\".format(i) for i in range(num_entities)]\n",
    "    ind2entity, entity2ind = build_dicts(entities)\n",
    "\n",
    "    relations = [\"<r_{}>\".format(i) for i in range(num_relations)]\n",
    "    ind2relation, relation2ind = build_dicts(relations)\n",
    "\n",
    "    atomic_dict = dict()   \n",
    "    atomic_facts = []\n",
    "    atomics = []\n",
    "\n",
    "    for i in tqdm(range(num_entities)):\n",
    "        # for each subject entity, randomly select some outgoing relations to some random object entity\n",
    "        num_rows = out_degree\n",
    "        selected_rows = np.random.choice(num_relations, size=num_rows, replace=False).tolist()\n",
    "        for row_idx in selected_rows:\n",
    "            col_idx = np.random.randint(num_entities)  # pick some random tail entity for each selected (h,r)\n",
    "            h,r,t = ind2entity[i], ind2relation[row_idx], ind2entity[col_idx]\n",
    "            atomic_facts.append(form_items([h, r], t))\n",
    "            atomics.append((h,r,t))\n",
    "            if h not in atomic_dict:\n",
    "                atomic_dict[h] = []\n",
    "            atomic_dict[h].append((r, t))\n",
    "    if not split_train_inferred:\n",
    "        inferred_facts = []\n",
    "        for ent in tqdm(entities):\n",
    "            for (r1, b) in atomic_dict[ent]:\n",
    "                for (r2, t) in atomic_dict[b]:\n",
    "                    inferred_facts.append(form_items([ent, r1, r2], t))\n",
    "        return (entities, relations, id_atomic_facts, ood_atomic_facts, \n",
    "            train_inferred_facts, test_inferred_iid, test_inferred_ood,\n",
    "            atomic_dict, ID_facts, OOD_facts)\n",
    "    \n",
    "    # split ID/OOD\n",
    "    OOD_ratio = 0.05\n",
    "    OOD_facts, ID_facts = split(atomics, round(len(atomics)*OOD_ratio))\n",
    "    OOD_facts, ID_facts = set(OOD_facts), set(ID_facts)\n",
    "\n",
    "    id_atomic_facts = [form_items([h, r], t) for (h,r,t) in ID_facts]\n",
    "    ood_atomic_facts = [form_items([h, r], t) for (h,r,t) in OOD_facts]\n",
    "\n",
    "    train_inferred_facts, test_inferred_iid, test_inferred_ood = [], [], []\n",
    "    for ent in tqdm(entities):\n",
    "        for (r1, b) in atomic_dict[ent]:\n",
    "            for (r2, t) in atomic_dict[b]:\n",
    "                if (ent, r1, b) in OOD_facts or (b, r2, t) in OOD_facts:\n",
    "                    if (ent, r1, b) in OOD_facts and (b, r2, t) in OOD_facts:\n",
    "                        test_inferred_ood.append(form_items([ent, r1, r2], t))\n",
    "                    continue\n",
    "                if np.random.uniform() > 0.005:\n",
    "                    train_inferred_facts.append(form_items([ent, r1, r2], t))\n",
    "                else:\n",
    "                    test_inferred_iid.append(form_items([ent, r1, r2], t))\n",
    "\n",
    "    return (entities, relations, id_atomic_facts, ood_atomic_facts, \n",
    "            train_inferred_facts, test_inferred_iid, test_inferred_ood,\n",
    "            atomic_dict, ID_facts, OOD_facts)\n",
    "    \n",
    "# default parameters\n",
    "NUM_ENTITY_IN = 2000\n",
    "NUM_RELATION = 200\n",
    "out_degree = 20\n",
    "(train_entities, train_relations, id_atomic_facts, ood_atomic_facts, \n",
    " train_inferred_facts, test_inferred_iid, test_inferred_facts,\n",
    " atomic_dict, ID_facts, OOD_facts) = build_dataset(\n",
    "    NUM_ENTITY_IN, NUM_RELATION, out_degree=out_degree, split_train_inferred=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2206\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "vocab = vocab + train_entities + train_relations\n",
    "# special tokens\n",
    "vocab = vocab + [\"<mask>\", \"<sep>\", \"<a>\", \"</a>\", \"<q>\", \"</q>\"]\n",
    "assert len(vocab) == len(set(vocab))\n",
    "print(\"vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 3000\n",
    "id_atomic_facts_ds = choose(id_atomic_facts, test_size)\n",
    "ood_atomic_facts_ds = choose(ood_atomic_facts, test_size)\n",
    "test_inferred_iid = choose(test_inferred_iid, test_size)\n",
    "test_inferred_facts_ds = choose(test_inferred_facts, test_size)\n",
    "\n",
    "all_atomics = id_atomic_facts + ood_atomic_facts\n",
    "len(all_atomics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "FACT-LEVEL AUGMENTATION\n",
      "==========================================================================================\n",
      "seed=42\n",
      "hr->t collisions (atomic_dict): 0\n",
      "test_ood parsed: hop1_ood_facts=1273, hop2_ood_facts=1284, bad_parse=0\n",
      "config:\n",
      "  hop1_ood_fact_ratio=0.0, hop1_samples_per_fact=0\n",
      "  hop2_ood_fact_ratio=0.0, hop2_samples_per_fact=0\n",
      "added:\n",
      "  +0 (uses OOD hop1 fact)\n",
      "  +0 (uses OOD hop2 fact)\n",
      "total new inferred: 0\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "=== Saved dataset: composition.2000.200.18.0_factaug_h1ratio0.0_h1k0_h2ratio0.0_h2k0 ===\n",
      "Train samples: 724000\n",
      "  - Atomic: 40000\n",
      "  - Inferred (augmented): 684000\n",
      "Valid samples: 2040\n",
      "Test probes: 13040\n",
      "============================================================\n",
      "\n",
      "==========================================================================================\n",
      "FACT-LEVEL AUGMENTATION\n",
      "==========================================================================================\n",
      "seed=42\n",
      "hr->t collisions (atomic_dict): 0\n",
      "test_ood parsed: hop1_ood_facts=1273, hop2_ood_facts=1284, bad_parse=0\n",
      "config:\n",
      "  hop1_ood_fact_ratio=1.0, hop1_samples_per_fact=18\n",
      "  hop2_ood_fact_ratio=1.0, hop2_samples_per_fact=18\n",
      "added:\n",
      "  +22697 (uses OOD hop1 fact)\n",
      "  +21492 (uses OOD hop2 fact)\n",
      "total new inferred: 44189\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "=== Saved dataset: composition.2000.200.18.0_factaug_h1ratio1.0_h1k18_h2ratio1.0_h2k18 ===\n",
      "Train samples: 768189\n",
      "  - Atomic: 40000\n",
      "  - Inferred (augmented): 728189\n",
      "Valid samples: 2040\n",
      "Test probes: 13040\n",
      "============================================================\n",
      "\n",
      "==========================================================================================\n",
      "FACT-LEVEL AUGMENTATION\n",
      "==========================================================================================\n",
      "seed=42\n",
      "hr->t collisions (atomic_dict): 0\n",
      "test_ood parsed: hop1_ood_facts=1273, hop2_ood_facts=1284, bad_parse=0\n",
      "config:\n",
      "  hop1_ood_fact_ratio=0.5, hop1_samples_per_fact=9\n",
      "  hop2_ood_fact_ratio=0.5, hop2_samples_per_fact=9\n",
      "added:\n",
      "  +5724 (uses OOD hop1 fact)\n",
      "  +5778 (uses OOD hop2 fact)\n",
      "total new inferred: 11502\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "=== Saved dataset: composition.2000.200.18.0_factaug_h1ratio0.5_h1k9_h2ratio0.5_h2k9 ===\n",
      "Train samples: 735502\n",
      "  - Atomic: 40000\n",
      "  - Inferred (augmented): 695502\n",
      "Valid samples: 2040\n",
      "Test probes: 13040\n",
      "============================================================\n",
      "\n",
      "==========================================================================================\n",
      "FACT-LEVEL AUGMENTATION\n",
      "==========================================================================================\n",
      "seed=42\n",
      "hr->t collisions (atomic_dict): 0\n",
      "test_ood parsed: hop1_ood_facts=1273, hop2_ood_facts=1284, bad_parse=0\n",
      "config:\n",
      "  hop1_ood_fact_ratio=1.0, hop1_samples_per_fact=18\n",
      "  hop2_ood_fact_ratio=0.0, hop2_samples_per_fact=0\n",
      "added:\n",
      "  +22697 (uses OOD hop1 fact)\n",
      "  +0 (uses OOD hop2 fact)\n",
      "total new inferred: 22697\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "=== Saved dataset: composition.2000.200.18.0_factaug_h1ratio1.0_h1k18_h2ratio0.0_h2k0 ===\n",
      "Train samples: 746697\n",
      "  - Atomic: 40000\n",
      "  - Inferred (augmented): 706697\n",
      "Valid samples: 2040\n",
      "Test probes: 13040\n",
      "============================================================\n",
      "\n",
      "==========================================================================================\n",
      "FACT-LEVEL AUGMENTATION\n",
      "==========================================================================================\n",
      "seed=42\n",
      "hr->t collisions (atomic_dict): 0\n",
      "test_ood parsed: hop1_ood_facts=1273, hop2_ood_facts=1284, bad_parse=0\n",
      "config:\n",
      "  hop1_ood_fact_ratio=0.0, hop1_samples_per_fact=0\n",
      "  hop2_ood_fact_ratio=1.0, hop2_samples_per_fact=18\n",
      "added:\n",
      "  +0 (uses OOD hop1 fact)\n",
      "  +21492 (uses OOD hop2 fact)\n",
      "total new inferred: 21492\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "=== Saved dataset: composition.2000.200.18.0_factaug_h1ratio0.0_h1k0_h2ratio1.0_h2k18 ===\n",
      "Train samples: 745492\n",
      "  - Atomic: 40000\n",
      "  - Inferred (augmented): 705492\n",
      "Valid samples: 2040\n",
      "Test probes: 13040\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from augmentation_utils import augment_train_inferred_with_fact_control, enhanced_fact_exposure_analysis\n",
    "\n",
    "parameters = [\n",
    "    # 0: No augmentation (Natural Grokking)\n",
    "    {\"HOP1_OOD_FACT_RATIO\": 0.0, \"HOP1_SAMPLES_PER_FACT\": 0,\n",
    "     \"HOP2_OOD_FACT_RATIO\": 0.0, \"HOP2_SAMPLES_PER_FACT\": 0},\n",
    "    \n",
    "    # 1: Both hop full augmentation \n",
    "    {\"HOP1_OOD_FACT_RATIO\": 1.0, \"HOP1_SAMPLES_PER_FACT\": 18,\n",
    "     \"HOP2_OOD_FACT_RATIO\": 1.0, \"HOP2_SAMPLES_PER_FACT\": 18},\n",
    "    \n",
    "    # 2: hop1/hop2 both hop half augmentation\n",
    "    {\"HOP1_OOD_FACT_RATIO\": 0.5, \"HOP1_SAMPLES_PER_FACT\": 9,\n",
    "     \"HOP2_OOD_FACT_RATIO\": 0.5, \"HOP2_SAMPLES_PER_FACT\": 9},\n",
    "    \n",
    "    # 3: hop 1 full augmentation only  \n",
    "    {\"HOP1_OOD_FACT_RATIO\": 1.0, \"HOP1_SAMPLES_PER_FACT\": 18,\n",
    "     \"HOP2_OOD_FACT_RATIO\": 0.0, \"HOP2_SAMPLES_PER_FACT\": 0},\n",
    "\n",
    "    # 4: hop 2 full augmentation only  \n",
    "    {\"HOP1_OOD_FACT_RATIO\": 0.0, \"HOP1_SAMPLES_PER_FACT\": 0,\n",
    "     \"HOP2_OOD_FACT_RATIO\": 1.0, \"HOP2_SAMPLES_PER_FACT\": 18},\n",
    "    \n",
    "]\n",
    "\n",
    "for aug_index in range(len(parameters)):\n",
    "    aug_config_index = aug_index\n",
    "    config = parameters[aug_config_index]\n",
    "\n",
    "    HOP1_OOD_FACT_RATIO = config[\"HOP1_OOD_FACT_RATIO\"]\n",
    "    HOP1_SAMPLES_PER_FACT = config[\"HOP1_SAMPLES_PER_FACT\"]\n",
    "    HOP2_OOD_FACT_RATIO = config[\"HOP2_OOD_FACT_RATIO\"]\n",
    "    HOP2_SAMPLES_PER_FACT = config[\"HOP2_SAMPLES_PER_FACT\"]\n",
    "\n",
    "    for phi in [18.0]:  \n",
    "        dataset_name = (\n",
    "            f\"composition.{NUM_ENTITY_IN}.{NUM_RELATION}.{phi}\"\n",
    "            f\"_factaug_h1ratio{HOP1_OOD_FACT_RATIO}_h1k{HOP1_SAMPLES_PER_FACT}\"\n",
    "            f\"_h2ratio{HOP2_OOD_FACT_RATIO}_h2k{HOP2_SAMPLES_PER_FACT}\"\n",
    "        )\n",
    "        os.makedirs(f\"data/{dataset_name}\", exist_ok=True)\n",
    "\n",
    "        # Step 5.1: Downsample train_inferred_facts\n",
    "        train_inferred_facts_ds = choose(train_inferred_facts, round(phi * len(id_atomic_facts)))\n",
    "\n",
    "        # Step 5.2: Fact-level augmentation\n",
    "        train_inferred_facts_augmented = augment_train_inferred_with_fact_control(\n",
    "            train_inferred_facts=train_inferred_facts_ds,\n",
    "            test_inferred_ood_ds=test_inferred_facts_ds,  # valid/test_ood sub sample set\n",
    "            atomic_dict=atomic_dict,\n",
    "            ID_facts=ID_facts,\n",
    "            OOD_facts=OOD_facts,\n",
    "            hop1_ood_fact_ratio=HOP1_OOD_FACT_RATIO,\n",
    "            hop1_samples_per_fact=HOP1_SAMPLES_PER_FACT,\n",
    "            hop2_ood_fact_ratio=HOP2_OOD_FACT_RATIO,\n",
    "            hop2_samples_per_fact=HOP2_SAMPLES_PER_FACT,\n",
    "            avoid_exposing_ood_test_bridges_when_hop2_injection_off=False,\n",
    "            seed=42,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        # Step 5.3: probes\n",
    "        probes = []\n",
    "        for item in id_atomic_facts_ds:\n",
    "            probes.append(deepcopy(item))\n",
    "            probes[-1][\"type\"] = \"id_atomic\"\n",
    "\n",
    "        for item in ood_atomic_facts_ds:\n",
    "            probes.append(deepcopy(item))\n",
    "            probes[-1][\"type\"] = \"ood_atomic\"\n",
    "\n",
    "        for item in choose(train_inferred_facts_augmented, test_size):\n",
    "            probes.append(deepcopy(item))\n",
    "            probes[-1][\"type\"] = \"train_inferred\"\n",
    "\n",
    "        for item in test_inferred_iid:\n",
    "            probes.append(deepcopy(item))\n",
    "            probes[-1][\"type\"] = \"test_inferred_iid\"\n",
    "\n",
    "        for item in test_inferred_facts_ds:\n",
    "            probes.append(deepcopy(item))\n",
    "            probes[-1][\"type\"] = \"test_inferred_ood\"\n",
    "\n",
    "        # Step 5.4: save files\n",
    "        with open(f\"data/{dataset_name}/train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_atomics + train_inferred_facts_augmented, f)\n",
    "\n",
    "        with open(f\"data/{dataset_name}/valid.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(test_inferred_facts_ds, f)\n",
    "\n",
    "        with open(f\"data/{dataset_name}/test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(probes, f)\n",
    "\n",
    "        with open(f\"data/{dataset_name}/vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(vocab, f)\n",
    "\n",
    "        with open(f\"data/{dataset_name}/id_facts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump([list(x) for x in ID_facts], f)\n",
    "\n",
    "        with open(f\"data/{dataset_name}/ood_facts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump([list(x) for x in OOD_facts], f)\n",
    "\n",
    "        print(f\"\\n=== Saved dataset: {dataset_name} ===\")\n",
    "        print(f\"Train samples: {len(all_atomics) + len(train_inferred_facts_augmented)}\")\n",
    "        print(f\"  - Atomic: {len(all_atomics)}\")\n",
    "        print(f\"  - Inferred (augmented): {len(train_inferred_facts_augmented)}\")\n",
    "        print(f\"Valid samples: {len(test_inferred_facts_ds)}\")\n",
    "        print(f\"Test probes: {len(probes)}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    # from augmentation_utils import iid_fact_count_report\n",
    "    # enhanced_fact_exposure_analysis(f\"./data/{dataset_name}\")\n",
    "    # iid_fact_count_report(f\"./data/{dataset_name}\", split=\"iid\")\n",
    "    # iid_fact_count_report(f\"./data/{dataset_name}\", split=\"ood\")\n",
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the following command to train with traditional Transformer first (Change datasetdir before training parameter sharing transformers)\n",
    "\n",
    "- ```cd Grokking_analysis && PYTHONPATH=\"$PWD/simpletransformers:${PYTHONPATH}\" PYTHONNOUSERSITE=1 CUDA_VISIBLE_DEVICES=0 python main.py --data_dir data/composition.2000.200.18.0_factaug_h1ratio0.5_h1k9_h2ratio0.5_h2k9 --model_name_or_path gpt2 --weight_decay 0.01 --output_dir output/composition.2000.200.18.0_factaug_h1ratio0.5_h1k9_h2ratio0.5_h2k9 --max_seq_length 10 --max_length 10 --block_size 10 --train_batch_size 512 --eval_batch_size 512 --learning_rate 1e-4 --gradient_accumulation_steps 1 --save_step 50000 --save_step_dense 40000 --max_steps 1500000 --do_train --scheduler constant_schedule_with_warmup --fp16 --evaluate_during_training --predict_during_training --init_weights --add_tokens --n_layer 4 --evaluate_train```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
