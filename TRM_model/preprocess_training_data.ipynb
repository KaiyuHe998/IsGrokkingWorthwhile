{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c4ea78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "_TOKEN_RE = re.compile(r\"<[^>]+>\")\n",
    "\n",
    "def augment_dataset_with_new_atomic(  # function for fine tuning dataset\n",
    "    old_data_path: str,\n",
    "    new_data_path: str,\n",
    "    *,\n",
    "    num_new_atomic: int = 2000,\n",
    "    keep_old_train: int = 8000,\n",
    "    cap_test_inferred_new: int = 3000,\n",
    "    cap_test_inferred_mix: int = 3000,\n",
    "    rng_seed: int = 0,\n",
    "    overwrite: bool = True,\n",
    "    verify_atomic_inferred: bool = True,\n",
    "    verify_print_topk: int = 10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a new dataset by keeping a subset of the old train set and adding new atomic facts.\n",
    "\n",
    "    Behavior is equivalent to the previous implementation with `half_ood=False`:\n",
    "    - Output is written to `new_data_path`.\n",
    "    - `test_inferred_new_mix` only composes with kept old atomic facts so that every old fact\n",
    "      referenced by the mix split is guaranteed to be present in the new train set.\n",
    "    \"\"\"\n",
    "    rng = random.Random(rng_seed)\n",
    "\n",
    "    out_data_path = new_data_path\n",
    "\n",
    "    if os.path.abspath(old_data_path) == os.path.abspath(out_data_path):\n",
    "        raise ValueError(\n",
    "            \"old_data_path must not be the same as the output path; otherwise it would overwrite the original dataset.\"\n",
    "        )\n",
    "\n",
    "    def load_json(p):\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def dump_json(obj, p):\n",
    "        with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(obj, f)\n",
    "\n",
    "    def toks(s: str):\n",
    "        return _TOKEN_RE.findall(s)\n",
    "\n",
    "    def make_atomic(h: str, r: str, t: str):\n",
    "        inp = f\"{h}{r}\"\n",
    "        return {\"input_text\": inp, \"target_text\": f\"{inp}{t}</a>\"}\n",
    "\n",
    "    def make_inferred(a: str, r1: str, r2: str, t: str):\n",
    "        inp = f\"{a}{r1}{r2}\"\n",
    "        return {\"input_text\": inp, \"target_text\": f\"{inp}{t}</a>\"}\n",
    "\n",
    "    # ---- load old dataset ----\n",
    "    train_path = os.path.join(old_data_path, \"train.json\")\n",
    "    valid_path = os.path.join(old_data_path, \"valid.json\")\n",
    "    test_path = os.path.join(old_data_path, \"test.json\")\n",
    "    vocab_path = os.path.join(old_data_path, \"vocab.json\")\n",
    "\n",
    "    if not (\n",
    "        os.path.exists(train_path)\n",
    "        and os.path.exists(valid_path)\n",
    "        and os.path.exists(test_path)\n",
    "        and os.path.exists(vocab_path)\n",
    "    ):\n",
    "        raise FileNotFoundError(\n",
    "            \"old_data_path must contain train.json, valid.json, test.json, vocab.json\"\n",
    "        )\n",
    "\n",
    "    old_train_full = load_json(train_path)\n",
    "    old_valid = load_json(valid_path)\n",
    "    old_test = load_json(test_path)\n",
    "    vocab_raw = load_json(vocab_path)\n",
    "\n",
    "    # Vocab compatibility: vocab.json can be either list[token] or dict[token->id]\n",
    "    if isinstance(vocab_raw, dict):\n",
    "        vocab_tokens = list(vocab_raw.keys())\n",
    "    else:\n",
    "        vocab_tokens = list(vocab_raw)\n",
    "\n",
    "    entities = [t for t in vocab_tokens if isinstance(t, str) and t.startswith(\"<e_\")]\n",
    "    relations = [t for t in vocab_tokens if isinstance(t, str) and t.startswith(\"<r_\")]\n",
    "    if not entities or not relations:\n",
    "        raise ValueError(\n",
    "            \"Failed to parse entities/relations from vocab.json (expected <e_*/<r_*> tokens).\"\n",
    "        )\n",
    "\n",
    "    # ---- split old train into atomic vs inferred ----\n",
    "    old_atomic_items = []\n",
    "    old_inferred_items = []\n",
    "    for item in old_train_full:\n",
    "        itoks = toks(item.get(\"input_text\", \"\"))\n",
    "        if len(itoks) == 2:\n",
    "            old_atomic_items.append(item)\n",
    "        elif len(itoks) == 3:\n",
    "            old_inferred_items.append(item)\n",
    "\n",
    "    # ---- build atomic map: (h,r) -> t ----\n",
    "    atomic_key_to_item = {}\n",
    "    atomic_map = {}\n",
    "    used_pairs = set()\n",
    "    used_rels_by_head = defaultdict(set)\n",
    "\n",
    "    # Keep a full copy of old facts to avoid collisions when generating new edges.\n",
    "    old_atomic_out = defaultdict(list)\n",
    "    old_atomic_edges = []\n",
    "\n",
    "    unresolved_atomic = 0\n",
    "    for item in old_atomic_items:\n",
    "        it = item.get(\"input_text\", \"\")\n",
    "        tt = item.get(\"target_text\", \"\")\n",
    "        itoks = toks(it)\n",
    "        ttoks = toks(tt)\n",
    "        if len(itoks) != 2 or len(ttoks) < 3:\n",
    "            unresolved_atomic += 1\n",
    "            continue\n",
    "        h, r = itoks[0], itoks[1]\n",
    "        t = ttoks[2]\n",
    "        key = (h, r)\n",
    "\n",
    "        if key not in atomic_key_to_item:\n",
    "            atomic_key_to_item[key] = item\n",
    "        if key not in atomic_map:\n",
    "            atomic_map[key] = t\n",
    "\n",
    "        used_pairs.add(key)\n",
    "        used_rels_by_head[h].add(r)\n",
    "        old_atomic_out[h].append((r, t))\n",
    "        old_atomic_edges.append((h, r, t))\n",
    "\n",
    "    if not atomic_key_to_item:\n",
    "        raise RuntimeError(\n",
    "            \"No valid atomic facts parsed from old train (expected len(tokens)==2).\"\n",
    "        )\n",
    "\n",
    "    # ---- build dependency: atomic_key -> inferred indices that use it (at least one hop) ----\n",
    "    atomic_key_to_inferred = defaultdict(set)\n",
    "    unresolved_inferred = 0\n",
    "\n",
    "    for idx, item in enumerate(old_inferred_items):\n",
    "        it = item.get(\"input_text\", \"\")\n",
    "        tt = item.get(\"target_text\", \"\")\n",
    "        itoks = toks(it)\n",
    "        ttoks = toks(tt)\n",
    "\n",
    "        if len(itoks) != 3 or len(ttoks) < 4:\n",
    "            unresolved_inferred += 1\n",
    "            continue\n",
    "\n",
    "        a, r1, r2 = itoks[0], itoks[1], itoks[2]\n",
    "        key1 = (a, r1)\n",
    "        b = atomic_map.get(key1, None)\n",
    "        if b is None:\n",
    "            atomic_key_to_inferred[key1].add(idx)\n",
    "            unresolved_inferred += 1\n",
    "            continue\n",
    "\n",
    "        key2 = (b, r2)\n",
    "        atomic_key_to_inferred[key1].add(idx)\n",
    "        atomic_key_to_inferred[key2].add(idx)\n",
    "\n",
    "    # ---- select old train by sampling atomic facts with closure of inferred ----\n",
    "    all_atomic_keys = list(atomic_key_to_item.keys())\n",
    "    rng.shuffle(all_atomic_keys)\n",
    "\n",
    "    selected_atomic = set()\n",
    "    selected_inferred = set()\n",
    "    total_selected = 0\n",
    "\n",
    "    def marginal_cost(key):\n",
    "        add_atomic = 0 if key in selected_atomic else 1\n",
    "        add_inferred = atomic_key_to_inferred.get(key, set()) - selected_inferred\n",
    "        return add_atomic + len(add_inferred), add_atomic, add_inferred\n",
    "\n",
    "    for key in all_atomic_keys:\n",
    "        cost, _, add_inferred = marginal_cost(key)\n",
    "        if total_selected + cost <= keep_old_train:\n",
    "            selected_atomic.add(key)\n",
    "            selected_inferred |= add_inferred\n",
    "            total_selected += cost\n",
    "            if total_selected == keep_old_train:\n",
    "                break\n",
    "\n",
    "    if total_selected < keep_old_train:\n",
    "        remaining = [k for k in all_atomic_keys if k not in selected_atomic]\n",
    "        rng.shuffle(remaining)\n",
    "        remaining.sort(key=lambda k: marginal_cost(k)[0])\n",
    "\n",
    "        for key in remaining:\n",
    "            cost, _, add_inferred = marginal_cost(key)\n",
    "            if cost == 0:\n",
    "                continue\n",
    "            if total_selected + cost <= keep_old_train:\n",
    "                selected_atomic.add(key)\n",
    "                selected_inferred |= add_inferred\n",
    "                total_selected += cost\n",
    "                if total_selected == keep_old_train:\n",
    "                    break\n",
    "\n",
    "    # ---- verify: closure for selected atomic keys ----\n",
    "    if verify_atomic_inferred:\n",
    "        missing_total = 0\n",
    "        zero_dep = 0\n",
    "        dep_sizes = []\n",
    "        worst_missing = 0\n",
    "        worst_key = None\n",
    "\n",
    "        for key in selected_atomic:\n",
    "            deps = atomic_key_to_inferred.get(key, set())\n",
    "            dep_sizes.append(len(deps))\n",
    "            if len(deps) == 0:\n",
    "                zero_dep += 1\n",
    "            missing = deps - selected_inferred\n",
    "            if missing:\n",
    "                missing_total += len(missing)\n",
    "                if len(missing) > worst_missing:\n",
    "                    worst_missing = len(missing)\n",
    "                    worst_key = key\n",
    "\n",
    "        print(\n",
    "            \"[verify] selected_atomic:\",\n",
    "            len(selected_atomic),\n",
    "            \"selected_inferred:\",\n",
    "            len(selected_inferred),\n",
    "            \"total_selected:\",\n",
    "            total_selected,\n",
    "            f\"(target={keep_old_train})\",\n",
    "        )\n",
    "        if dep_sizes:\n",
    "            dep_sizes_sorted = sorted(dep_sizes, reverse=True)\n",
    "            print(\n",
    "                \"[verify] atomic->inferred dependency sizes:\",\n",
    "                f\"min={min(dep_sizes)}, median={dep_sizes_sorted[len(dep_sizes)//2]}, max={max(dep_sizes)}\",\n",
    "            )\n",
    "            print(\"[verify] selected atomic with dep_size==0:\", zero_dep)\n",
    "\n",
    "        if missing_total != 0:\n",
    "            raise RuntimeError(\n",
    "                f\"[verify] Missing closure: selected_atomic has inferred dependencies that were not included. \"\n",
    "                f\"missing_total={missing_total}, worst_key={worst_key}, worst_missing={worst_missing}\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"[verify] OK: For each selected atomic key, all dependent train inferred items (>=1 hop) are included.\"\n",
    "            )\n",
    "\n",
    "        if verify_print_topk > 0:\n",
    "            sample_keys = list(selected_atomic)\n",
    "            rng.shuffle(sample_keys)\n",
    "            sample_keys = sample_keys[:verify_print_topk]\n",
    "            for k in sample_keys:\n",
    "                deps = atomic_key_to_inferred.get(k, set())\n",
    "                print(f\"[verify] sample atomic_key={k} dep_inferred_count={len(deps)}\")\n",
    "\n",
    "    # ---- assemble old_train_kept ----\n",
    "    old_train_kept = []\n",
    "    for key in selected_atomic:\n",
    "        old_train_kept.append(atomic_key_to_item[key])\n",
    "    for idx in sorted(selected_inferred):\n",
    "        old_train_kept.append(old_inferred_items[idx])\n",
    "    rng.shuffle(old_train_kept)\n",
    "\n",
    "    # Only build structures from old atomic facts that are actually kept in train,\n",
    "    # so that every old fact referenced by test_inferred_new_mix is guaranteed to be present in train.\n",
    "    kept_old_atomic_out = defaultdict(list)\n",
    "    kept_old_atomic_edges = []\n",
    "    for (h, r) in selected_atomic:\n",
    "        t = atomic_map.get((h, r), None)\n",
    "        if t is None:\n",
    "            continue\n",
    "        kept_old_atomic_out[h].append((r, t))\n",
    "        kept_old_atomic_edges.append((h, r, t))\n",
    "\n",
    "    # ---- old test keys for dedup ----\n",
    "    old_test_key = set()\n",
    "    for item in old_test:\n",
    "        old_test_key.add((item.get(\"input_text\"), item.get(\"target_text\"), item.get(\"type\")))\n",
    "\n",
    "    # ---- generate new atomic edges ----\n",
    "    new_edges = []\n",
    "    new_pairs = set()\n",
    "    new_rels_by_head = defaultdict(set)\n",
    "\n",
    "    def pick_unused_relation(head: str):\n",
    "        used = used_rels_by_head.get(head, set()) | new_rels_by_head.get(head, set())\n",
    "        candidates = [r for r in relations if r not in used]\n",
    "        if not candidates:\n",
    "            return None\n",
    "        return rng.choice(candidates)\n",
    "\n",
    "    target_pairs = num_new_atomic // 2\n",
    "    attempts = 0\n",
    "    while len(new_edges) < 2 * target_pairs and attempts < 500000:\n",
    "        attempts += 1\n",
    "        a = rng.choice(entities)\n",
    "        b = rng.choice(entities)\n",
    "        t = rng.choice(entities)\n",
    "\n",
    "        r1 = pick_unused_relation(a)\n",
    "        r2 = pick_unused_relation(b)\n",
    "        if r1 is None or r2 is None:\n",
    "            continue\n",
    "        if (a, r1) in used_pairs or (a, r1) in new_pairs:\n",
    "            continue\n",
    "        if (b, r2) in used_pairs or (b, r2) in new_pairs:\n",
    "            continue\n",
    "\n",
    "        new_edges.append((a, r1, b))\n",
    "        new_pairs.add((a, r1))\n",
    "        new_rels_by_head[a].add(r1)\n",
    "\n",
    "        new_edges.append((b, r2, t))\n",
    "        new_pairs.add((b, r2))\n",
    "        new_rels_by_head[b].add(r2)\n",
    "\n",
    "    attempts = 0\n",
    "    while len(new_edges) < num_new_atomic and attempts < 500000:\n",
    "        attempts += 1\n",
    "        h = rng.choice(entities)\n",
    "        r = pick_unused_relation(h)\n",
    "        if r is None:\n",
    "            continue\n",
    "        if (h, r) in used_pairs or (h, r) in new_pairs:\n",
    "            continue\n",
    "        t = rng.choice(entities)\n",
    "\n",
    "        new_edges.append((h, r, t))\n",
    "        new_pairs.add((h, r))\n",
    "        new_rels_by_head[h].add(r)\n",
    "\n",
    "    if len(new_edges) < num_new_atomic:\n",
    "        raise RuntimeError(\n",
    "            f\"Only generated {len(new_edges)} new atomic facts (constraints may be too strict / not enough relations).\"\n",
    "        )\n",
    "\n",
    "    # ---- build new train ----\n",
    "    new_atomic_items = [make_atomic(h, r, t) for (h, r, t) in new_edges]\n",
    "    new_train = list(old_train_kept) + new_atomic_items\n",
    "\n",
    "    # ---- build inferred probes ----\n",
    "    new_out = defaultdict(list)\n",
    "    for (h, r, t) in new_edges:\n",
    "        new_out[h].append((r, t))\n",
    "\n",
    "    inferred_new = []\n",
    "    for (a, r1, b) in new_edges:\n",
    "        for (r2, t) in new_out.get(b, []):\n",
    "            inferred_new.append(make_inferred(a, r1, r2, t))\n",
    "\n",
    "    # ---- test_inferred_new_mix (restricted to kept old facts to guarantee coverage in train) ----\n",
    "    inferred_mix = []\n",
    "    # new -> old\n",
    "    for (a, r1, b) in new_edges:\n",
    "        for (r2, t) in kept_old_atomic_out.get(b, []):\n",
    "            inferred_mix.append(make_inferred(a, r1, r2, t))\n",
    "    # old -> new\n",
    "    for (a, r1, b) in kept_old_atomic_edges:\n",
    "        for (r2, t) in new_out.get(b, []):\n",
    "            inferred_mix.append(make_inferred(a, r1, r2, t))\n",
    "\n",
    "    rng.shuffle(inferred_new)\n",
    "    rng.shuffle(inferred_mix)\n",
    "    inferred_new = inferred_new[:cap_test_inferred_new]\n",
    "    inferred_mix = inferred_mix[:cap_test_inferred_mix]\n",
    "\n",
    "    if len(inferred_mix) < cap_test_inferred_mix:\n",
    "        print(\n",
    "            \"[augment_dataset_with_new_atomic] warning: inferred_mix only\",\n",
    "            len(inferred_mix),\n",
    "            \"< cap_test_inferred_mix=\",\n",
    "            cap_test_inferred_mix,\n",
    "            \"(mix now restricted to kept old facts to guarantee coverage in train)\",\n",
    "        )\n",
    "\n",
    "    # ---- assemble new test ----\n",
    "    new_test = list(old_test)\n",
    "\n",
    "    for item in new_atomic_items:\n",
    "        probe = dict(item)\n",
    "        probe[\"type\"] = \"atomic_new\"\n",
    "        key = (probe[\"input_text\"], probe[\"target_text\"], probe[\"type\"])\n",
    "        if key not in old_test_key:\n",
    "            new_test.append(probe)\n",
    "            old_test_key.add(key)\n",
    "\n",
    "    for item in inferred_new:\n",
    "        probe = dict(item)\n",
    "        probe[\"type\"] = \"test_inferred_new\"\n",
    "        key = (probe[\"input_text\"], probe[\"target_text\"], probe[\"type\"])\n",
    "        if key not in old_test_key:\n",
    "            new_test.append(probe)\n",
    "            old_test_key.add(key)\n",
    "\n",
    "    for item in inferred_mix:\n",
    "        probe = dict(item)\n",
    "        probe[\"type\"] = \"test_inferred_new_mix\"\n",
    "        key = (probe[\"input_text\"], probe[\"target_text\"], probe[\"type\"])\n",
    "        if key not in old_test_key:\n",
    "            new_test.append(probe)\n",
    "            old_test_key.add(key)\n",
    "\n",
    "    # ---- write new dataset (overwrite) ----\n",
    "    os.makedirs(out_data_path, exist_ok=True)\n",
    "\n",
    "    if overwrite:\n",
    "        for fn in [\"train.json\", \"valid.json\", \"test.json\", \"vocab.json\"]:\n",
    "            fp = os.path.join(out_data_path, fn)\n",
    "            if os.path.exists(fp):\n",
    "                os.remove(fp)\n",
    "\n",
    "    dump_json(new_train, os.path.join(out_data_path, \"train.json\"))\n",
    "    dump_json(old_valid, os.path.join(out_data_path, \"valid.json\"))\n",
    "    dump_json(new_test, os.path.join(out_data_path, \"test.json\"))\n",
    "    dump_json(vocab_raw, os.path.join(out_data_path, \"vocab.json\"))\n",
    "\n",
    "    # ---- prints ----\n",
    "    print(\"[augment_dataset_with_new_atomic] old_train_full:\", len(old_train_full))\n",
    "    print(\n",
    "        \"[augment_dataset_with_new_atomic] parsed atomic:\",\n",
    "        len(old_atomic_items),\n",
    "        \"parsed inferred:\",\n",
    "        len(old_inferred_items),\n",
    "    )\n",
    "    print(\"[augment_dataset_with_new_atomic] kept old:\", len(old_train_kept), f\"(target={keep_old_train})\")\n",
    "    print(\"[augment_dataset_with_new_atomic] added new atomic:\", len(new_atomic_items), f\"(target={num_new_atomic})\")\n",
    "    print(\"[augment_dataset_with_new_atomic] new train total:\", len(new_train))\n",
    "    print(\n",
    "        \"[augment_dataset_with_new_atomic] test additions (after caps):\",\n",
    "        \"atomic_new=\",\n",
    "        len(new_atomic_items),\n",
    "        \"test_inferred_new=\",\n",
    "        len(inferred_new),\n",
    "        \"test_inferred_new_mix=\",\n",
    "        len(inferred_mix),\n",
    "    )\n",
    "\n",
    "    if unresolved_atomic or unresolved_inferred:\n",
    "        print(\n",
    "            \"[augment_dataset_with_new_atomic] warnings:\",\n",
    "            f\"unresolved_atomic={unresolved_atomic}, unresolved_inferred={unresolved_inferred}\",\n",
    "        )\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "def preprocess_data(data_path, save_path):\n",
    "    max_seq_len = 5\n",
    "    all_input_ids = []\n",
    "    all_output_ids = []\n",
    "    puzzle_index = [0]\n",
    "    group_index = [0]\n",
    "    puzzle_identifier = []\n",
    "    group_index = [0]\n",
    "    puzzle_identifier = []\n",
    "    with open(data_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    for index, data_piece in enumerate(data):\n",
    "        input_text = data_piece['input_text']\n",
    "        output_text = data_piece['target_text']\n",
    "        input_tokens = tokenizer.tokenize(input_text)\n",
    "        output_tokens = tokenizer.tokenize(output_text)\n",
    "        input_ids = [int(vocab_map[token]) for token in input_tokens]\n",
    "        output_ids = [int(vocab_map[token]) for token in output_tokens]\n",
    "        if len(input_ids) < max_seq_len:\n",
    "            pad_len = max_seq_len - len(input_ids)\n",
    "            pad_seq = [0] * pad_len\n",
    "            input_ids.extend(pad_seq)\n",
    "        if len(output_ids) < max_seq_len:\n",
    "            pad_len = max_seq_len - len(output_ids)\n",
    "            pad_seq = [0] * pad_len\n",
    "            output_ids.extend(pad_seq)\n",
    "        # print(f\"{index}/{len(data)}\", end=\"\\r\")\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_output_ids.append(output_ids)\n",
    "        puzzle_index.append(index + 1)\n",
    "        group_index.append(index + 1)\n",
    "        puzzle_identifier.append(0)\n",
    "    all_input_ids = np.array(all_input_ids, dtype=np.int32)\n",
    "    all_output_ids = np.array(all_output_ids, dtype=np.int32)\n",
    "    puzzle_index = np.array(puzzle_index, dtype=np.int32)\n",
    "    group_index = np.array(group_index, dtype=np.int32)\n",
    "    dataset_json = {\n",
    "        \"pad_id\": 0,\n",
    "        \"ignore_label_id\": 0,\n",
    "        \"blank_identifier_id\": 0,\n",
    "        \"vocab_size\": len(vocab_map),\n",
    "        \"seq_len\": max_seq_len,\n",
    "        \"num_puzzle_identifiers\": 1,\n",
    "        \"total_groups\": len(data),\n",
    "        \"mean_puzzle_examples\": 1.0,\n",
    "        \"total_puzzles\": len(data),\n",
    "        \"sets\": [\"all\"],\n",
    "    }\n",
    "    np.save(os.path.join(save_path, 'all__group_indices.npy'), group_index)\n",
    "    np.save(os.path.join(save_path, 'all__inputs.npy'), all_input_ids)\n",
    "    np.save(os.path.join(save_path, 'all__labels.npy'), all_output_ids)\n",
    "    np.save(os.path.join(save_path, 'all__puzzle_identifiers.npy'), puzzle_identifier)\n",
    "    np.save(os.path.join(save_path, 'all__puzzle_indices.npy'), puzzle_index)\n",
    "    with open(os.path.join(save_path, 'dataset.json'), 'w') as f:\n",
    "        json.dump(dataset_json, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c404da9c",
   "metadata": {},
   "source": [
    "# Pre-training data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c42b6b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Checkpoint path: ../Grokking_analysis/output/composition.2000.200.18.0_factaug_h1ratio0.5_h1k9_h2ratio0.5_h2k9/checkpoint-2000\n",
      "‚úì Check if path exists: True\n",
      "\n",
      "================================================================================\n",
      "Method 1: Using transformers.GPT2Tokenizer.from_pretrained()\n",
      "================================================================================\n",
      "‚úÖ Tokenizer loaded successfully!\n",
      "   Vocabulary size: 52463\n",
      "   Special tokens: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n",
      "\n",
      "üß™ Test Tokenization:\n",
      "   Input text: <e_0><r_1><a>e_2</a>\n",
      "   Tokens: ['<e_0>', '<r_1>', '<a>', 'e', '_', '2', '</a>']\n",
      "   Token IDs: [50257, 52258, 52459, 68, 62, 17, 52460]\n",
      "   Decoded text: <e_0> <r_1> <a> e_2 </a>\n",
      "\n",
      "================================================================================\n",
      "Method 2: Manually build mapping from vocab.json\n",
      "================================================================================\n",
      "‚úÖ Vocabulary loaded successfully!\n",
      "   Vocabulary size: 50257\n",
      "   First 10 tokens: ['!', '!!', '!!!', '!!!!', '!!!!!', '!!!!!!!!', '!!\"', '!\"', '!\",', '!\".']\n",
      "   Last 10 tokens: ['ƒº√©ƒ®ƒ¥', 'ƒΩ', 'ƒæ', 'ƒø', '≈Ä', '≈Å', '≈Ç', '≈É', '≈É¬∑', '≈Éƒ∂']\n",
      "\n",
      "üß™ Test manual mapping:\n",
      "   Tokens: ['<e_0>', '<r_1>', '<a>']\n",
      "   IDs: [0, 0, 0]\n",
      "   Decoded: ['!', '!', '!']\n",
      "\n",
      "================================================================================\n",
      "Method 3: View all files in the checkpoint\n",
      "================================================================================\n",
      "üìã Files in the checkpoint:\n",
      "   - added_tokens.json              (      44,886 bytes)\n",
      "   - all_items.json                 (   2,097,911 bytes)\n",
      "   - config.json                    (         970 bytes)\n",
      "   - generation_config.json         (         124 bytes)\n",
      "   - logit_lens_ood.json            (  18,893,584 bytes)\n",
      "   - merges.txt                     (     456,318 bytes)\n",
      "   - model.safetensors              ( 277,732,416 bytes)\n",
      "   - model_args.json                (       2,866 bytes)\n",
      "   - optimizer.pt                   ( 555,497,797 bytes)\n",
      "   - scheduler.pt                   (         627 bytes)\n",
      "   - special_tokens_map.json        (         470 bytes)\n",
      "   - tokenizer_config.json          (     382,926 bytes)\n",
      "   - training_args.bin              (       3,771 bytes)\n",
      "   - vocab.json                     (     999,186 bytes)\n",
      "\n",
      "================================================================================\n",
      "üéØ Special Token Analysis\n",
      "================================================================================\n",
      "Dataset-defined special tokens:\n",
      "\n",
      "üìä Token type statistics:\n",
      "   Entity tokens (<e_*): 0\n",
      "   Relation tokens (<r_*): 0\n",
      "   Special tokens: 6\n",
      "   Total: 50257\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# specify your datapath (Please make sure you have already trained on traditional Transformer)\n",
    "# dataset_name = 'composition.2000.200.18.0_factaug_h1ratio0.0_h1k0_h2ratio0.0_h2k0' # Natrual Grokking\n",
    "dataset_name = 'composition.2000.200.18.0_factaug_h1ratio0.5_h1k9_h2ratio0.5_h2k9' \n",
    "data_root_path_name = f'../Grokking_analysis/data/{dataset_name}'\n",
    "\n",
    "checkpoint_path = f\"../Grokking_analysis/output/{dataset_name}/checkpoint-2000\"\n",
    "\n",
    "print(f\"\\nüìÇ Checkpoint path: {checkpoint_path}\")\n",
    "print(f\"‚úì Check if path exists: {os.path.exists(checkpoint_path)}\")\n",
    "\n",
    "# Method 1: Load using transformers directly\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Method 1: Using transformers.GPT2Tokenizer.from_pretrained()\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "try:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(checkpoint_path)\n",
    "    print(f\"‚úÖ Tokenizer loaded successfully!\")\n",
    "    print(f\"   Vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"   Special tokens: {tokenizer.special_tokens_map}\")\n",
    "    \n",
    "    # Test tokenization\n",
    "    test_text = \"<e_0><r_1><a>e_2</a>\"\n",
    "    tokens = tokenizer.tokenize(test_text)\n",
    "    token_ids = tokenizer.encode(test_text)\n",
    "    \n",
    "    print(f\"\\nüß™ Test Tokenization:\")\n",
    "    print(f\"   Input text: {test_text}\")\n",
    "    print(f\"   Tokens: {tokens}\")\n",
    "    print(f\"   Token IDs: {token_ids}\")\n",
    "    print(f\"   Decoded text: {tokenizer.decode(token_ids)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Loading failed: {e}\")\n",
    "\n",
    "# Method 2: Manually load vocab.json (lower-level approach)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Method 2: Manually build mapping from vocab.json\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "vocab_path = os.path.join(checkpoint_path, \"vocab.json\")\n",
    "with open(vocab_path, \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# ÊûÑÂª∫ token <-> id Êò†Â∞Ñ\n",
    "token2id = vocab  # vocab.json Â∑≤ÁªèÊòØ {token: id} Ê†ºÂºè\n",
    "id2token = {v: k for k, v in token2id.items()}\n",
    "\n",
    "print(f\"‚úÖ Vocabulary loaded successfully!\")\n",
    "print(f\"   Vocabulary size: {len(token2id)}\")\n",
    "print(f\"   First 10 tokens: {list(token2id.keys())[:10]}\")\n",
    "print(f\"   Last 10 tokens: {list(token2id.keys())[-10:]}\")\n",
    "\n",
    "# Test manual mapping\n",
    "test_tokens = [\"<e_0>\", \"<r_1>\", \"<a>\"]\n",
    "test_ids = [token2id.get(t, token2id.get(\"<unk>\", 0)) for t in test_tokens]\n",
    "decoded_tokens = [id2token.get(i, \"<unk>\") for i in test_ids]\n",
    "\n",
    "print(f\"\\nüß™ Test manual mapping:\")\n",
    "print(f\"   Tokens: {test_tokens}\")\n",
    "print(f\"   IDs: {test_ids}\")\n",
    "print(f\"   Decoded: {decoded_tokens}\")\n",
    "\n",
    "# Method 3: View all files in the checkpoint\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Method 3: View all files in the checkpoint\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "checkpoint_files = os.listdir(checkpoint_path)\n",
    "print(f\"üìã Files in the checkpoint:\")\n",
    "for f in sorted(checkpoint_files):\n",
    "    file_path = os.path.join(checkpoint_path, f)\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"   - {f:30s} ({size:>12,} bytes)\")\n",
    "\n",
    "# Special token information\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üéØ Special Token Analysis\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "special_tokens = [\"<mask>\", \"<sep>\", \"<a>\", \"</a>\", \"<q>\", \"</q>\"]\n",
    "print(f\"Dataset-defined special tokens:\")\n",
    "for token in special_tokens:\n",
    "    if token in token2id:\n",
    "        print(f\"   {token:10s} ‚Üí ID: {token2id[token]}\")\n",
    "\n",
    "# Entity and relation token statistics\n",
    "entity_tokens = [k for k in token2id.keys() if k.startswith(\"<e_\")]\n",
    "relation_tokens = [k for k in token2id.keys() if k.startswith(\"<r_\")]\n",
    "print(f\"\\nüìä Token type statistics:\")\n",
    "print(f\"   Entity tokens (<e_*): {len(entity_tokens)}\")\n",
    "print(f\"   Relation tokens (<r_*): {len(relation_tokens)}\")\n",
    "print(f\"   Special tokens: {len(special_tokens)}\")\n",
    "print(f\"   Total: {len(token2id)}\")\n",
    "\n",
    "\n",
    "vocab_map = {'<PAD>':0}\n",
    "with open(f\"{data_root_path_name}/vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "vocab_map = {token: idx+1 for idx, token in enumerate(vocab)}\n",
    "reverse_vocab_map = {idx: token for token, idx in vocab_map.items()}\n",
    "if not os.path.exists(f\"{data_root_path_name}/reverse_vocab_map.json\"):\n",
    "    with open(f\"{data_root_path_name}/reverse_vocab_map.json\", \"w\") as f:\n",
    "        json.dump(reverse_vocab_map, f)\n",
    "\n",
    "if not os.path.exists(f\"{data_root_path_name}/vocab_map.json\"):\n",
    "    with open(f\"{data_root_path_name}/vocab_map.json\", \"w\") as f:\n",
    "        json.dump(vocab_map, f)\n",
    "        \n",
    "\n",
    "preprocess_data(f\"{data_root_path_name}/test.json\",\n",
    "                f'data/{dataset_name}/test')\n",
    "preprocess_data(f\"{data_root_path_name}/train.json\",\n",
    "                f'data/{dataset_name}/train')\n",
    "preprocess_data(f\"{data_root_path_name}/valid.json\",\n",
    "                f'data/{dataset_name}/valid')\n",
    "\n",
    "identifiers_file = [\"<blank>\"]\n",
    "with open(f'data/{dataset_name}/identifiers.json', 'w') as f:\n",
    "    json.dump(identifiers_file, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634408b-8659-400e-b472-48bbfbd3f708",
   "metadata": {},
   "source": [
    "### Using the following code to finetuning on the training model\n",
    "\n",
    "- ```run_name=\"pretrain_grok_composition\" && CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_grok_evaluate_ver_0_1.py   arch=trm   data_paths=\"[data/composition.2000.200.18.0_factaug_h1ratio0.5_h1k9_h2ratio0.5_h2k9]\"   evaluators=\"[]\"   epochs=1100   eval_interval=5   lr=4e-5   puzzle_emb_lr=1e-4   weight_decay=1.0   puzzle_emb_weight_decay=1.0   arch.mlp_t=True   arch.pos_encodings=None   arch.L_layers=2   arch.H_cycles=2   arch.L_cycles=6 arch.halt_max_steps=1 arch.hidden_size=1536 +run_name=${run_name}  ema=True   global_batch_size=512  +max_inference_steps=1 checkpoint_every_eval=True   +format=\"maintain_prefix\" +causal=False +post_fix=\"anything_here_you_like\"```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778d8fc-e05e-40e6-8b34-40b44e9b3c52",
   "metadata": {},
   "source": [
    "# Finetuning data preprocess (Please make sure you have already trained with traditional Transformer on origional dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62f127d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[verify] selected_atomic: 220 selected_inferred: 7780 total_selected: 8000 (target=8000)\n",
      "[verify] atomic->inferred dependency sizes: min=0, median=37, max=48\n",
      "[verify] selected atomic with dep_size==0: 4\n",
      "[verify] OK: For each selected atomic key, all dependent train inferred items (>=1 hop) are included.\n",
      "[verify] sample atomic_key=('<e_286>', '<r_147>') dep_inferred_count=41\n",
      "[verify] sample atomic_key=('<e_632>', '<r_21>') dep_inferred_count=30\n",
      "[verify] sample atomic_key=('<e_1853>', '<r_71>') dep_inferred_count=45\n",
      "[verify] sample atomic_key=('<e_209>', '<r_86>') dep_inferred_count=28\n",
      "[verify] sample atomic_key=('<e_491>', '<r_178>') dep_inferred_count=0\n",
      "[verify] sample atomic_key=('<e_1097>', '<r_115>') dep_inferred_count=41\n",
      "[verify] sample atomic_key=('<e_233>', '<r_96>') dep_inferred_count=32\n",
      "[verify] sample atomic_key=('<e_905>', '<r_179>') dep_inferred_count=39\n",
      "[verify] sample atomic_key=('<e_1674>', '<r_8>') dep_inferred_count=37\n",
      "[verify] sample atomic_key=('<e_1768>', '<r_177>') dep_inferred_count=42\n",
      "[augment_dataset_with_new_atomic] warning: inferred_mix only 459 < cap_test_inferred_mix= 3000 (mix now restricted to kept old facts to guarantee coverage in train)\n",
      "[augment_dataset_with_new_atomic] old_train_full: 735502\n",
      "[augment_dataset_with_new_atomic] parsed atomic: 40000 parsed inferred: 695502\n",
      "[augment_dataset_with_new_atomic] kept old: 8000 (target=8000)\n",
      "[augment_dataset_with_new_atomic] added new atomic: 2000 (target=2000)\n",
      "[augment_dataset_with_new_atomic] new train total: 10000\n",
      "[augment_dataset_with_new_atomic] test additions (after caps): atomic_new= 2000 test_inferred_new= 3000 test_inferred_new_mix= 459\n",
      "2000 3000 459\n",
      "================================================================================\n",
      "üîß Load Tokenizer from checkpoints\n",
      "================================================================================\n",
      "\n",
      "üß™ test mapping:\n",
      "   Tokens: ['<e_0>', '<r_1>', '<a>']\n",
      "   IDs: [0, 0, 0]\n",
      "   decode: ['!', '!', '!']\n",
      "üìã Checkpoint files:\n",
      "   - added_tokens.json              (      44,886 bytes)\n",
      "   - all_items.json                 (   2,097,911 bytes)\n",
      "   - config.json                    (         970 bytes)\n",
      "   - generation_config.json         (         124 bytes)\n",
      "   - logit_lens_ood.json            (  18,893,584 bytes)\n",
      "   - merges.txt                     (     456,318 bytes)\n",
      "   - model.safetensors              ( 277,732,416 bytes)\n",
      "   - model_args.json                (       2,866 bytes)\n",
      "   - optimizer.pt                   ( 555,497,797 bytes)\n",
      "   - scheduler.pt                   (         627 bytes)\n",
      "   - special_tokens_map.json        (         470 bytes)\n",
      "   - tokenizer_config.json          (     382,926 bytes)\n",
      "   - training_args.bin              (       3,771 bytes)\n",
      "   - vocab.json                     (     999,186 bytes)\n",
      "\n",
      "================================================================================\n",
      "üéØ special Token analysis\n",
      "================================================================================\n",
      "special token:\n",
      "\n",
      "üìä Token type statics:\n",
      "   entity token (<e_*): 0\n",
      "   relation token (<r_*): 0\n",
      "   special token: 6\n",
      "   total token count: 50257\n"
     ]
    }
   ],
   "source": [
    "# add new non-seen facts into the dataset (No augmentation version)\n",
    "# dataset_name = 'composition.2000.200.18.0_factaug_h1ratio0.0_h1k0_h2ratio0.0_h2k0' # Natrual Grokking\n",
    "dataset_name = 'composition.2000.200.18.0_factaug_h1ratio0.5_h1k9_h2ratio0.5_h2k9' \n",
    "old_data_path = f'../Grokking_analysis/data/{dataset_name}' # origional dataset path\n",
    "new_data_path = old_data_path+'_finetuning'\n",
    "\n",
    "augment_dataset_with_new_atomic(old_data_path,new_data_path)\n",
    "with open(f'{new_data_path}/test.json','r') as f:\n",
    "    asd = json.load(f)\n",
    "new_atomic_count = 0\n",
    "new_infer_count = 0\n",
    "mix_infer_count = 0\n",
    "\n",
    "for i in asd:\n",
    "    if (i['type'] == 'test_inferred_new'):\n",
    "        new_infer_count += 1\n",
    "    if (i['type'] == 'test_inferred_new_mix'):\n",
    "        mix_infer_count += 1\n",
    "    if (i['type'] == 'atomic_new'):\n",
    "        new_atomic_count += 1\n",
    "print(new_atomic_count, new_infer_count, mix_infer_count)\n",
    "\n",
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "print(\"=\"*80)\n",
    "print(\"üîß Load Tokenizer from checkpoints\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "data_root_path_name = new_data_path.split('/')[-1] # the datapath need to be transferred\n",
    "old_data_folder_name = old_data_path.split('/')[-1]\n",
    "\n",
    "checkpoint_path = f\"../Grokking_analysis/output/{dataset_name}/checkpoint-2000\" \n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "\n",
    "vocab_path = os.path.join(checkpoint_path, \"vocab.json\")\n",
    "with open(vocab_path, \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# construct token <-> id mapping\n",
    "token2id = vocab  # vocab.json already in {token: id} format\n",
    "id2token = {v: k for k, v in token2id.items()}\n",
    "\n",
    "\n",
    "\n",
    "# test mapping\n",
    "test_tokens = [\"<e_0>\", \"<r_1>\", \"<a>\"]\n",
    "test_ids = [token2id.get(t, token2id.get(\"<unk>\", 0)) for t in test_tokens]\n",
    "decoded_tokens = [id2token.get(i, \"<unk>\") for i in test_ids]\n",
    "\n",
    "print(f\"\\nüß™ test mapping:\")\n",
    "print(f\"   Tokens: {test_tokens}\")\n",
    "print(f\"   IDs: {test_ids}\")\n",
    "print(f\"   decode: {decoded_tokens}\")\n",
    "\n",
    "\n",
    "checkpoint_files = os.listdir(checkpoint_path)\n",
    "print(f\"üìã Checkpoint files:\")\n",
    "for f in sorted(checkpoint_files):\n",
    "    file_path = os.path.join(checkpoint_path, f)\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"   - {f:30s} ({size:>12,} bytes)\")\n",
    "\n",
    "# speacial token information\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üéØ special Token analysis\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "special_tokens = [\"<mask>\", \"<sep>\", \"<a>\", \"</a>\", \"<q>\", \"</q>\"]\n",
    "print(f\"special token:\")\n",
    "for token in special_tokens:\n",
    "    if token in token2id:\n",
    "        print(f\"   {token:10s} ‚Üí ID: {token2id[token]}\")\n",
    "\n",
    "# entity relation token statics\n",
    "entity_tokens = [k for k in token2id.keys() if k.startswith(\"<e_\")]\n",
    "relation_tokens = [k for k in token2id.keys() if k.startswith(\"<r_\")]\n",
    "print(f\"\\nüìä Token type statics:\")\n",
    "print(f\"   entity token (<e_*): {len(entity_tokens)}\")\n",
    "print(f\"   relation token (<r_*): {len(relation_tokens)}\")\n",
    "print(f\"   special token: {len(special_tokens)}\")\n",
    "print(f\"   total token count: {len(token2id)}\")\n",
    "\n",
    "\n",
    "vocab_map = {'<PAD>':0}\n",
    "with open(f\"../Grokking_analysis/data/{data_root_path_name}/vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "vocab_map = {token: idx+1 for idx, token in enumerate(vocab)}\n",
    "reverse_vocab_map = {idx: token for token, idx in vocab_map.items()}\n",
    "if not os.path.exists(f\"../Grokking_analysis/data/{data_root_path_name}/reverse_vocab_map.json\"):\n",
    "    with open(f\"../Grokking_analysis/data/{data_root_path_name}/reverse_vocab_map.json\", \"w\") as f:\n",
    "        json.dump(reverse_vocab_map, f)\n",
    "\n",
    "if not os.path.exists(f\"../Grokking_analysis/data/{data_root_path_name}/vocab_map.json\"):\n",
    "    with open(f\"../Grokking_analysis/data/{data_root_path_name}/vocab_map.json\", \"w\") as f:\n",
    "        json.dump(vocab_map, f)\n",
    "        \n",
    "\n",
    "preprocess_data(f\"../Grokking_analysis/data/{data_root_path_name}/test.json\",\n",
    "                f'data/{data_root_path_name}/test')\n",
    "preprocess_data(f\"../Grokking_analysis/data/{data_root_path_name}/train.json\",\n",
    "                f'data/{data_root_path_name}/train')\n",
    "preprocess_data(f\"../Grokking_analysis/data/{data_root_path_name}/valid.json\",\n",
    "                f'data/{data_root_path_name}/valid')\n",
    "\n",
    "identifiers_file = [\"<blank>\"]\n",
    "with open(f'/home/kxh230002/TRM_LCM/TinyRecursiveModels/data/grokking/{data_root_path_name}/identifiers.json', 'w') as f:\n",
    "    json.dump(identifiers_file, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83142dd-e1b6-4442-a724-17911c175ef7",
   "metadata": {},
   "source": [
    "### Using the following code to finetuning a model\n",
    "- ```run_name=\"TRM_finetune\" && LOAD_CKPT={Your TRM checkpoint path here} && CUDA_VISIBLE_DEVICES=0 torchrun --nproc-per-node=1 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain_grok_evaluate_ver_0_1.py arch=trm data_paths=\"[{Your finetune datapath here}]\" evaluators=\"[]\" +load_checkpoint=\"${LOAD_CKPT}\" epochs=20000 eval_interval=50 lr=2e-5 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle_emb_weight_decay=1.0 arch.mlp_t=True arch.pos_encodings=None arch.L_layers=2 arch.H_cycles=2 arch.L_cycles=6 arch.halt_max_steps=1 arch.hidden_size=1536 ema=True global_batch_size=512 +max_inference_steps=1 checkpoint_every_eval=False +causal=False +run_name=\"${run_name}\" +post_fix=\"2000.200.18.0_no_aug_finetuning\"``` (Keep the archtecture parameters the same to your pretrain checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explainableLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
